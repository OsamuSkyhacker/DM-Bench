# Backprop Benchmark

This directory contains a Backprop benchmark derived from Rodinia with two implementations:
- `Managed/`: Uses CUDA Unified Memory and supports strategies such as `cudaMemAdvise` / `cudaMemPrefetchAsync`.
- `UnManaged/`: Uses explicit device memory allocation and copy.

Both versions include wall-clock timing. At the end of execution the program prints: `Total elapsed time: X.XXX s`.

## Workload Overview

### Mathematical Model & Training Flow
- Fully-connected feed-forward neural network with one hidden layer; the activation function is typically sigmoid (the program focuses on lightweight floating-point operations).
- Forward pass: `h = σ(W_in→hid^T · x)`, `y = σ(W_hid→out^T · h)`.
- Backward pass: Compute error terms via the chain rule and update weights, in the form `W ← W − η · ∇W`.
- This benchmark focuses on compute/memory-access performance and VRAM consumption. It does not involve complex data augmentation or advanced optimizers.

### GPU Parallel Mapping
- Thread granularity: Each thread processes several neurons/tiles; block dimension is controlled by the compile-time macro `RD_WG_SIZE_0` (default 16).
- Block reduction: A block-level reduction is used for hidden-layer gradients, producing about `ceil(hid/32)·hid` temporary buffer.
- Memory layout: Weights and intermediate activations are stored as contiguous float arrays. The dominant VRAM term grows linearly with `in` and `hid` (see “Memory vs. Parameters”).
- UM strategy (Managed): Access hints and explicit prefetch (`AB/RM/PL/PF`) can be combined as needed.

### Parameters & Inputs
- Runtime args: `<in>` (input size, must be a multiple of 16), `[hid]` (hidden size, multiples of 32 recommended).
- Compile-time macros: `RD_WG_SIZE_0` (parallel granularity), `HOST_TRANSFER_ONLY` / `WRITE_RESULT` (output control).
- Program input: No external files; samples are generated by `imagenet.c` for reproducibility and scalability.

## Directory Structure
- `Managed/`
  - `backprop.c`, `backprop_cuda.cu`, `backprop_cuda_kernel.cu`, `imagenet.c`, `backprop.h`
  - `Makefile`, `Makefile_nvidia`
  - `run` (example script)
- `UnManaged/`
  - Same-named source files
  - `Makefile`, `Makefile_nvidia`
  - `run`

## Build

- Default (single stream):
```bash
make -C Managed      # build Managed/backprop
make -C UnManaged    # build UnManaged/backprop
```

- Clean:
```bash
make -C Managed clean
make -C UnManaged clean
```

- Compile-time options (example shows Managed; UnManaged passes through the same way)
  - Block dimension: `-DRD_WG_SIZE_0=<16|32|...>` (default 16)
  - Output control: `-DHOST_TRANSFER_ONLY` (copy back to host only, no file) or `-DWRITE_RESULT` (copy back and write `result.txt`, overwrite if exists)
  - Note: Backprop currently has no compile-time multi-stream switch (unlike BFS’s `-DMULTISTREAM`). For multi-stream prefetch/overlap, refer to `BFS/Managed` and extend as needed.

Example:
```bash
make -C Managed CFLAGS="-DRD_WG_SIZE_0=32 -DWRITE_RESULT"
```

## Run

### Managed

Executable: `Managed/backprop`

Usage:
```bash
./backprop <in> [hid] [AB dev|cpu] [RM dev|cpu] [PL dev|cpu] [PF dev|cpu]
```
Arguments:
- `<in>`: Input layer size, must be divisible by 16 (ensures `blocks_y = in/16`).
- `[hid]`: Hidden neurons, multiples of 32 recommended; default 2048.
- `AB/RM/PL/PF`: Independent UM strategy switches, freely combinable; `dev` is the device id (e.g. `0`), `cpu` means host.

UM parameter notes (non-exclusive, can be combined):
- `AB dev|cpu`: `cudaMemAdviseSetAccessedBy`, specify the accessor.
- `RM dev|cpu`: `cudaMemAdviseSetReadMostly`, hint that region is mostly read-only.
- `PL dev|cpu`: `cudaMemAdviseSetPreferredLocation`, suggest initial residency.
- `PF dev|cpu`: `cudaMemPrefetchAsync`, single-stream prefetch then sync; `dev` is device id, `cpu` is host.

Examples:
```bash
cd Managed
# ≈12.5 GiB: in=1,048,560, hid=1,600, enable AB+PF to GPU0
./backprop 1048560 1600 AB 0 PF 0
# ≈20 GiB: in=1,048,560, hid=2,560, prefer CPU residence and prefetch to GPU0
./backprop 1048560 2560 PL cpu PF 0
```

Recommended VRAM (fixed `in=1,048,560`, meeting the upper bound `in/16=65535`):

| Target VRAM | Suggested `in` | Suggested `hid` |
| --- | --- | --- |
| 1 GiB | 1,048,560 | 128 |
| 4 GiB | 1,048,560 | 512 |
| 8 GiB | 1,048,560 | 1,024 |
| 12 GiB | 1,048,560 | 1,536 |
| 16 GiB | 1,048,560 | 2,048 |
| 20 GiB | 1,048,560 | 2,560 |

(`hid` is already 32-aligned; feel free to increase for headroom.)

### UnManaged

Executable: `UnManaged/backprop`

Usage:
```bash
./backprop <in> [hid]
```
Example (same suggestions as Managed):
```bash
cd UnManaged
./backprop 1048560 1600
```

## Program Input (none here)

No external data files are required. Samples are generated by `imagenet.c`.

## Program Output

- Stdout: Training progress and `Total elapsed time: X.XXX s` (wall-clock).
- File output (optional):
  - With `-DWRITE_RESULT` (or alias `-DDUMP_RESULT`) the result is written to `result.txt` (overwrite if exists, create if not).
  - `-DHOST_TRANSFER_ONLY` copies back to host only, no file; useful to avoid I/O noise during performance testing.

## Memory vs. Parameters (estimation)

Let `in = <input size>`, `hid = <hidden size>`, `sizeof(float)=4`. Dominant terms are the same for Managed and UnManaged:
```text
Mem_bytes ≈ 4 × [ 2·(in+1)(hid+1)   // two copies of weights (current & previous)
                 + (in+1)           // input vector
                 + 2·(hid+1)        // hidden outputs + hidden deltas
                 + ceil(hid/32)·hid // partial reduction buffer
               ]
```
When `in` and `hid` are large, the dominant term is `8·in·hid`, thus:
```text
Mem_GiB ≈ 8·in·hid / 2^30
```
Given a target VRAM `M_GiB` (with fixed `in`), the required hidden size is:
```text
hid_raw = (M_GiB · 2^30) / (8 · in)
hid = ceil(hid_raw / 32) · 32
```
Examples:
- `in=1,048,560`, `hid=1,600` → ≈12.5 GiB
- `in=1,048,560`, `hid=2,560` → ≈20.0 GiB

## Notes
- `in` must be divisible by 16; `hid` should be a multiple of 32.
- To avoid I/O interfering with performance tests, it is recommended not to write the result file by default (leave `WRITE_RESULT` off).
- For large-scale tests ensure sufficient GPU VRAM and, if needed, bind devices via `CUDA_VISIBLE_DEVICES`.
